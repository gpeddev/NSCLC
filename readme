How to test my code

Instructions:

                                            Prepare directories
0.Download Backup the data from the pcloud.
Link=> https://filedn.eu/l94VllPDHGB44tvSOamtqo4/Backup_Data.zip
1.Extract Backup_Data.zip and get Backup_Data folder.
2.Move Backup_Data folder to NSCLC->Data
3.Run Script->restore_from_backup.sh in order to fill the folders 00_Images and 01_Masks

                                            Preprocessing
4.Run Preprocessing.py in order to fill preprocess folders with the CTs. The results are storred on folders
from 02_WindowCT_VAE1_results to 08_CroppedWindow_VAE2_results. (See description of each folder)

                                            Tuning models
5.Run VAE_1_tunning_model_1.py or VAE_1_tunning_model_2a.py etc to train the corresponding model
Results are storred in Output folder.
In case you want to save the models run the Scripts->VAE_1_save_last_model.sh.
This will save the datasets, testing, validation, training, the encoder,decoder,VAE, a file with the hyperparameter values
Logs and Image folders of each model from each fold to the StoreResults folder within a folder with the current date.
In case VAE_1_tunning_model_1.py or VAE_1_tunning_model_2.py clean directories with VAE_1_clean_after_tunning.sh
before running another VAE_1 tunning models.

In the project VAE 1 tumor two models were tested with grid search corresponding to files VAE_1_model_1.py
and VAE_1_model_2a.py. The other models were not grid searched for various reasons.
These files draw their hyperparameters from the  VAE_1_model_1_parameters.py and  VAE_1_model_2a_parameters.py
in case you want to try them with other hyperparameters. Their current values are the optimized hyperparameters.


                                            Model comparison
6.After training the models are stored at Output folder. Comparison makes no sense based on loss function when
different kl weight is used. It is made with SSIM.
7.To use evaluate_vae_ssim function inside SSIM calculator.py point the variable model_path to the position
of the model. (Path) and if you are testing/evaluating VAE 1 use CropTumor Path variable as the directory to input
the images. CropTumor points to "./Data/05_CropTumor_VAE1_results/" 138x138 images
If you are testing/evaluating VAE 2 use CroppedWindow which points to "./Data/08_CroppedWindow_VAE2_results/" 206x206
images.
The boolean variable val at evaluate_vae_ssim decides if you want to test the validation dataset or the testing dataset

                                            Feature extraction (Pooling)
Pooling is performed at the beginning of Classification_LassoSmoteSVM.py
The pooling function is pooling(encoder_path, vaes_latent_space, source_images) in Pooling->Pooling.py

                                            Feature selection
Also happens in Classification_LassoSmoteSVM.py after pooling.

                                            SVM training and results
Also happens in Classification_LassoSmoteSVM.py after feature selection.

In order to run Classification_LassoSmoteSVM.py to produce results you must first prepare the clinical and gene data and
fix the appropriate values of Classification_LassoSmoteSVM.py
latent_space
baseline
model_path
image_source

8. To prepare clinical and gene data run DataCleaning.py. It should load the data from
"./ClinicalData/NSCLCR01Radiogenomic_DATA_LABELS_2018-05-22_1500-shifted.csv" perform cleaning and store them to
"./ClinicalDataAnalysis/data.csv"

9. To perform exploratory analysis run Exploratory_Analysis.py to produce the graphs for the report
(exploratory analysis section)

10. Now we can perform Feature extraction, Feature selection, and SVM and get the results.

after import there are 4 variables needed to run sucessfully the above
###########
latent_space        => fill with the model latent space you want to run. Info about it is in the model folder inside the
                       file hyperparameters.txt
baseline            => if you want to see the baseline result of the VAE (just VAE features) set it to True
                       else it integrates VAE features + clinical and gene data and produce the results.
model_path          => Set it to point to the main folder of the model you want to test.
image_source        => If its a VAE 1 and you import 138x138 images set it to CropTumor else
                       if its a VAE 2 and you import 206x206 images set it to CroppedWindow
In the last gridsearch i set tol=1e-3 because getting a final result with 1e-4 takes more than a day in my computer.
Also sometimes gridsearch freezes. In my computer I had to reboot it and everything worked.





Directories


BestResults=>
This is where I stored the best models from each grid search. It should be empty at your case since just the 4 models
are over 90GB.

ClinicalData=>
Should contain the file NSCLCR01Radiogenomic_DATA_LABELS_2018-05-22_1500-shifted.csv

ClinicalDataAnalysis=>
Should contain python source files DataCleaning.py and Exploratory_Analysis.py and when you run DataCleaning should
contain data.csv file.


Data=>
00_Images: CT scans
01_Masks: CT masks
02_WindowCT_VAE1_results: CT after applied window
03_GrayscaleCT_VAE1_results: CT after converted to uint8 and 0-255 range
04_MaskCT_VAE1_results: CTs after applied the mask
05_CropTumor_VAE1_results: CT after an area with the center of tumor is cropped
(size of the area the max length of tumor on all slices)
06_NewMaskWindow_VAE2_results: Expands the mask to a window (in order to crop a window around the tumor)
07_MaskedCT_VAE2_results: Applies the mask to the CT
08_CroppedWindow_VAE2_results: Crop the CTs of the above directory.

Final Models=>
Ignore this folder

Models=> (VAE_1 and VAE_2)
Folder that holds the code of the training models and their hyperparameters.
Images will be created when you run the files VAE_1_model_*.py inside the directory corresponding to the
model graphs presented in the report
Only VAE_1_model_1 and VAE_1_model_2a were presented on the report from VAE 1

Output=>
Folder that holds the results of the training network
The structure of the Output file is =>
Output->    VAE_1->DatasetSplits        # contains the training, validation and testing dataset
            VAE_1->Images               # When Visualization_VAE1.py is used it fills 0-4 folders with images from VAE.
                                        # the image you input and the output of the vae.
                                        # I used at the beginning to test the output but I havent used in the report.
                                        # or for a while since I searched for a more objective way to compare the output
                                        # I ended using SSIM. Should work if you point the appropriate paths to read
                                        # the models but you can ignore it.
            VAE_1->Images->0
            VAE_1->Images->1
            VAE_1->Images->2
            VAE_1->Images->3
            VAE_1->Images->4
            VAE_1->Logs                 # You can find the logs of the trained model here.
            VAE_1->Models               # You can find VAE_1.h5 to VAE_5.h5 here and the corresponding encoders and
                                        # decoders. Each one corresponds to the appropriate fold.
            VAE_1->hyperparameters.txt  # You can find some statistics here about the model and the hyperparameters used
            VAE_1->vae_1.png            # a models graph.
Output->    VAE_2->DatasetSplits        # contains the training, validation and testing dataset
            VAE_2->Images               # When Visualization_VAE2.py is used it fills 0-4 folders with images from VAE.
                                        # the image you input and the output of the vae.
                                        # I used at the beginning to test the output but I havent used in the report.
                                        # or for a while since I searched for a more objective way to compare the output
                                        # I ended using SSIM. Should work if you point the appropriate paths to read
                                        # the models but you can ignore it.
            VAE_2->Images->0
            VAE_2->Images->1
            VAE_2->Images->2
            VAE_2->Images->3
            VAE_2->Images->4
            VAE_2->Logs                 # You can find the logs of the trained model here.
            VAE_2->Models               # You can find VAE_1.h5 to VAE_5.h5 here and the corresponding encoders and
                                        # decoders. Each one corresponds to the appropriate fold.
            VAE_2->hyperparameters.txt  # You can find some statistics here about the model and the hyperparameters used
            VAE_2->vae_1.png            # a models graph.


Pooling->   Pooling.py                  # contains the pooling function

ProcessingCTs=>
Folder for the code of preprocessing the images

Scripts=>
clean_data.sh: Script to clean all contain of folders from 00_* until 10_*
restore_from_backup.sh: restore 00_Images and 01_Masks containt                 # This should be run
VAE_1_clean_after_tunning.sh: Clean Output folder
VAE_1_save_last_model.sh: Saves last model from Output to StoreResults
VAE_2_clean_after_tunning.sh: Clean Output folder
VAE_2_save_last_model.sh: Saves last model from Output to StoreResults

StoreResults=>
Folder to store the model. I used it to store the models I wanted to keep. It should be empty unless you plan to
train more than one models in which case the scripts *save_last_model.sh will store them here.

SupportCode=>
Paths and dataset handling code


Classification_LassoSmoteSVM.py=>
Does lasso feature selection, Smote for slightly balancing the classes and SVM for classification

Preprocessing.py=>
Run to create the contain of 00_Images until 08_CroppedWindow_VAE2_results

SSIM calculator.py=>
Code for comparing models

TestGPU.py=>
test if GPU is working

VAE_*_tunning_model_*.py=>
Correspond to the appropriate model in Models=> (VAE_1 and VAE_2) for tuning.

Visualization_VAE*.py =>
Fills the Output->VAE_*->Images->* with before after images. I leave it for testing purposes ignore it